{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d47b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nmshaik\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\nmshaik\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nmshaik\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nmshaik\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nmshaik\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nmshaik\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9222a3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T10:27:19.983692Z",
     "iopub.status.busy": "2025-08-24T10:27:19.980684Z",
     "iopub.status.idle": "2025-08-24T10:27:21.256309Z",
     "shell.execute_reply": "2025-08-24T10:27:21.247779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: clean_hf.csv\n",
      "  Rows in file: 1409\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "database is locked",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     96\u001b[39m         description = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m     features_meta.append((col, description, source, \u001b[33m\"\u001b[39m\u001b[33mv1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(datetime.now())))\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m cursor.executemany(\u001b[33m'''\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[33mINSERT OR IGNORE INTO feature_metadata \u001b[39m\n\u001b[32m    101\u001b[39m \u001b[33m(feature_name, description, source, version, created_at)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[33mVALUES (?, ?, ?, ?, ?)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[33m\u001b[39m\u001b[33m'''\u001b[39m, features_meta)\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Insert data into the feature store\u001b[39;00m\n\u001b[32m    106\u001b[39m records = df_to_insert.to_records(index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mOperationalError\u001b[39m: database is locked"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    Path(r\"D:\\BITS_SEM2\\DMML_Assignment\\Task4_DataPreparation\\processed_data\\clean_hf.csv\"),\n",
    "    Path(r\"D:\\BITS_SEM2\\DMML_Assignment\\Task4_DataPreparation\\processed_data\\clean_kaggle.csv\")\n",
    "]\n",
    "\n",
    "# Columns that exist in the SQLite feature store table\n",
    "table_cols = [\n",
    "    \"customerID\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\",\n",
    "    \"Contract_OneYear\", \"Contract_TwoYear\",\n",
    "    \"PaymentMethod_CreditCard\", \"PaymentMethod_ElectronicCheck\",\n",
    "    \"PaymentMethod_MailedCheck\", \"Churn\"\n",
    "]\n",
    "\n",
    "# Use context manager to safely open/close the DB\n",
    "with sqlite3.connect(\"feature_store.db\") as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # -------- Step 1: Create metadata table if not exists --------\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS feature_metadata (\n",
    "        feature_name TEXT PRIMARY KEY,\n",
    "        description TEXT,\n",
    "        source TEXT,\n",
    "        version TEXT,\n",
    "        created_at TEXT\n",
    "    )\n",
    "    ''')\n",
    "\n",
    "    # -------- Step 2: Create feature store table if not exists --------\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS feature_store (\n",
    "        customerID TEXT PRIMARY KEY,\n",
    "        tenure INTEGER,\n",
    "        MonthlyCharges REAL,\n",
    "        TotalCharges REAL,\n",
    "        Contract_OneYear INTEGER,\n",
    "        Contract_TwoYear INTEGER,\n",
    "        PaymentMethod_CreditCard INTEGER,\n",
    "        PaymentMethod_ElectronicCheck INTEGER,\n",
    "        PaymentMethod_MailedCheck INTEGER,\n",
    "        Churn INTEGER\n",
    "    )\n",
    "    ''')\n",
    "\n",
    "    # -------- Step 3: Loop through CSV files --------\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Processing: {csv_file.name}\")\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f\"  Rows in file: {len(df)}\")\n",
    "\n",
    "        # Standardize column names: strip spaces, lowercase\n",
    "        df.columns = df.columns.str.strip().str.replace(\" \", \"\").str.lower()\n",
    "        table_cols_lower = [col.lower() for col in table_cols]\n",
    "\n",
    "        # Keep only columns that exist in both CSV and table\n",
    "        df_to_insert = df[[col for col in table_cols_lower if col in df.columns]]\n",
    "\n",
    "        # Rename columns to match SQLite table\n",
    "        rename_map = dict(zip(df_to_insert.columns, [col for col in table_cols if col.lower() in df_to_insert.columns]))\n",
    "        df_to_insert = df_to_insert.rename(columns=rename_map)\n",
    "\n",
    "        # Ensure all table columns exist\n",
    "        for col in table_cols:\n",
    "            if col not in df_to_insert.columns:\n",
    "                if col == \"customerID\":\n",
    "                    df_to_insert[col] = \"\"\n",
    "                elif col == \"Churn\":\n",
    "                    df_to_insert[col] = 0\n",
    "                else:\n",
    "                    df_to_insert[col] = 0\n",
    "\n",
    "        # Reorder columns to match table exactly\n",
    "        df_to_insert = df_to_insert[table_cols]\n",
    "\n",
    "        # Insert metadata dynamically\n",
    "        features_meta = []\n",
    "        for col in df_to_insert.columns:\n",
    "            if col == \"Churn\":\n",
    "                source = \"Label\"\n",
    "                description = \"Whether customer churned (0/1)\"\n",
    "            elif col.startswith(\"Contract\") or col.startswith(\"PaymentMethod\"):\n",
    "                source = \"Engineered\"\n",
    "                description = f\"Feature: {col}\"\n",
    "            elif col == \"customerID\":\n",
    "                source = \"ID\"\n",
    "                description = \"Unique customer identifier\"\n",
    "            else:\n",
    "                source = \"Telco DB\"\n",
    "                description = f\"Feature: {col}\"\n",
    "            features_meta.append((col, description, source, \"v1\", str(datetime.now())))\n",
    "\n",
    "        # Insert metadata safely\n",
    "        try:\n",
    "            cursor.executemany('''\n",
    "            INSERT OR IGNORE INTO feature_metadata \n",
    "            (feature_name, description, source, version, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "            ''', features_meta)\n",
    "        except sqlite3.OperationalError as e:\n",
    "            print(\"Could not insert metadata:\", e)\n",
    "\n",
    "        # Insert data into the feature store\n",
    "        records = df_to_insert.to_records(index=False)\n",
    "        cursor.executemany('''\n",
    "        INSERT OR REPLACE INTO feature_store VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', records)\n",
    "\n",
    "    # -------- Step 4: Retrieve and display data --------\n",
    "    features = pd.read_sql(\"SELECT * FROM feature_store\", conn)\n",
    "    print(\"Feature Store Data:\")\n",
    "    display(features)\n",
    "\n",
    "    metadata = pd.read_sql(\"SELECT * FROM feature_metadata\", conn)\n",
    "    print(\"Feature Metadata:\")\n",
    "    display(metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
